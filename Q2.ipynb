{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectifs du travail**: Le but de ce travail pratique est de prédire le mot manquant dans un proverbe incomplet en utilisant un transformer. Il a aussi pour but d'évoluer la performance d'un transformer, en particulier Bert dans notre cas, avant et après fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installation et déclaration des variables**\n",
    "Pour la bonne exécution du travail, certaines dépendances sont installées.\n",
    "Pour ce travail, nous avons sélectionné le model bert-base-multilingual-uncased de Hugging face. Ce choix se justifie par le fait que notre corpus est en français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-23T20:50:18.234873Z",
     "iopub.status.busy": "2022-12-23T20:50:18.234203Z",
     "iopub.status.idle": "2022-12-23T20:51:24.204098Z",
     "shell.execute_reply": "2022-12-23T20:51:24.202896Z",
     "shell.execute_reply.started": "2022-12-23T20:50:18.234835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.7/site-packages (4.20.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (4.13.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.10.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (4.64.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2.28.1)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.19.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.1.97)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers[sentencepiece]) (3.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (1.26.12)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (0.0.53)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sacremoses) (4.64.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.15.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from sacremoses) (2021.11.10)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses) (4.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (3.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706dc0a04d62445aa3558d693a9256eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b734259ec67b42ac8841b602800ab4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/641M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc690f1c63c4ecd8a5857f1722d6fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d715a0e92644a3871f688ecc3eb484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install sacremoses\n",
    "from transformers import BertTokenizer, BertForMaskedLM, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "cuda_device = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_checkpoint = \"bert-base-multilingual-uncased\"\n",
    "model = BertForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fonctions utilitaires**:\n",
    "Les fonctions ***load_proverbs*** et ***load_tests*** servent à lire et récupérer respectivement les données pour l'entrainement et pour le test.\n",
    "La fonction ***score*** est utilisée pour calculer la perplexité d'un provebe test. Cette fonction utilise le tokeniseur pour encoder le proverbe. Après celà, chaque mot du proverbe est remplacé par le masque du modèle de manière itérative puis la perplexité est calculée. La perplexité du proverbe est alors assimilée à la somme de la perplexité de chaque mot.\n",
    "\n",
    "La fonction ***get_solutions*** permet de récupérer les solutions i.e. les vrais mots à trouver.\n",
    "\n",
    "La fonction ***get_predictions*** utilise la fonction ***score*** pour déterminer l'option la plus plausible. Pour ce faire, pour chaque proverbe incomplet, les 3 astérix sont remplacés par une option. Puis la perplexité du proverbe \"complété\" est calculée. L'option du proverbe avec la plus faible perplexité est considérée comme la solution la plus plausible. \n",
    "\n",
    "La fonction ***precision_metric*** donne la précision du modèle après la prédiction. Elle calcule le taux des proverbes bien prédits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-23T20:51:24.206812Z",
     "iopub.status.busy": "2022-12-23T20:51:24.206163Z",
     "iopub.status.idle": "2022-12-23T20:51:24.226075Z",
     "shell.execute_reply": "2022-12-23T20:51:24.221547Z",
     "shell.execute_reply.started": "2022-12-23T20:51:24.206781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Method to load train data from file\n",
    "def load_proverbs(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        raw_lines = f.readlines()\n",
    "    return [x.strip() for x in raw_lines]\n",
    "\n",
    "# Method to load test data from file\n",
    "def load_tests(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as fp:\n",
    "        test_data = json.load(fp)\n",
    "    return test_data\n",
    "\n",
    "def score(model, tokenizer, sentence):\n",
    "    tensor_input = tokenizer.encode(sentence, return_tensors='pt').to(device)\n",
    "    repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1).to(device)\n",
    "    mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2].to(device)\n",
    "    masked_input = repeat_input.masked_fill(mask == 1, tokenizer.mask_token_id).to(device)\n",
    "    labels = repeat_input.masked_fill( masked_input != tokenizer.mask_token_id, -100).to(device)\n",
    "    with torch.inference_mode():\n",
    "        loss = model(masked_input, labels=labels).loss\n",
    "    return np.exp(loss.item())\n",
    "\n",
    "def get_solutions(test_data):\n",
    "    return [data[\"solution\"] for data in test_data]\n",
    "\n",
    "def get_predictions(test_data, model, tokenizer):\n",
    "    predictions = []\n",
    "    for datas in test_data:\n",
    "        scores = {}\n",
    "        partial_proverb = datas['test']\n",
    "        for option in datas['choices']:\n",
    "            incomplet_proverb = partial_proverb.replace(\"***\", option)\n",
    "            scores[option] = score(model, tokenizer, incomplet_proverb)\n",
    "        good_sentence = min(scores, key=scores.get)\n",
    "        predictions.append(good_sentence)\n",
    "        scores.clear()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "    true_pred = 0\n",
    "\n",
    "    for idx in range(len(y_pred)):\n",
    "        if y_true[idx] == y_pred[idx]:\n",
    "            true_pred += 1\n",
    "    \n",
    "    return true_pred * 100 / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Sans fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-23T20:51:24.228726Z",
     "iopub.status.busy": "2022-12-23T20:51:24.227986Z",
     "iopub.status.idle": "2022-12-23T20:51:28.806481Z",
     "shell.execute_reply": "2022-12-23T20:51:28.805293Z",
     "shell.execute_reply.started": "2022-12-23T20:51:24.228688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision avant fine tuning est : 35.714285714285715\n"
     ]
    }
   ],
   "source": [
    "tests_proverbs = load_tests(\"data/test_proverbes.json\")\n",
    "word_predictions = get_predictions(tests_proverbs, model, tokenizer)\n",
    "right_words = get_solutions(tests_proverbs)\n",
    "print(f\"La précision avant fine tuning est : {precision_metric(right_words, word_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Fine tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ***prepare_data*** permet de préparer les données d'entrainement comme suit:\n",
    "* Tokenisation : le tokenizer de Bert ***BertTokenizer*** est utilisé pour tokeniser chaque proverbe du corpus. La taille des tokens est est fixée à 32. Dans le cas où la taille de tokens d'un proverb est supérieure à la taille limite, le vecteur de tokens est tronqué d'ouù le paramètre ***truncation=true***. Dans le cas où la taille du vecteur de tokens est inférieure alors un padding est appliqué pour atteindre la taille de 32.\n",
    "* Création des labels : au vecteur obtenu après la tokénisation, une colonne est ajoutée pour les labels. Ses valeurs sont une copie de des propriétés ***inputs_ids*** \n",
    "* Masquage : Nous avons appliqué la théorie de Bert qui consiste à masquer 15% des tokens du jeu de données. Le masquage consiste à remplacé un token d'un proverbe par ce token spécifique, MASK. Le masquage peut être appliqué à tout token dont l'index est différent de 101, 102. En effet, lors de la tokénisation, des tokens spécifiques (CLS et SEP) ont été ajoutés et les index 101 et 102 leur ont été attribués respectivement. L'index 103 est attribué au token MASK\n",
    "\n",
    "Les données obtenues après la préparation sont utilisées pour entrainer (ajuster) le modèle avec notre corpus. Nous avons utilisé Trainer et TrainingArguments de Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-23T20:51:28.809553Z",
     "iopub.status.busy": "2022-12-23T20:51:28.808921Z",
     "iopub.status.idle": "2022-12-23T20:51:29.944298Z",
     "shell.execute_reply": "2022-12-23T20:51:29.943300Z",
     "shell.execute_reply.started": "2022-12-23T20:51:28.809513Z"
    }
   },
   "outputs": [],
   "source": [
    "proverbs = load_proverbs(\"data/proverbes.txt\")\n",
    "\n",
    "def prepare_data(data_train):\n",
    "    inputs_train = tokenizer(data_train, return_tensors='pt', max_length=32, truncation=True, padding='max_length')\n",
    "    inputs_train['labels'] = inputs_train.input_ids.detach().clone()\n",
    "    # Create and add mask\n",
    "    rand_float = torch.rand(inputs_train.input_ids.shape)\n",
    "    mask_arr = (rand_float < 0.15) * (inputs_train.input_ids != 101) * (inputs_train.input_ids != 102) * (inputs_train.input_ids != 0)\n",
    "    selection = []\n",
    "\n",
    "    for i in range(inputs_train.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "\n",
    "    for i in range(inputs_train.input_ids.shape[0]):\n",
    "        inputs_train.input_ids[i, selection[i]] = 103\n",
    "        \n",
    "    return inputs_train\n",
    "\n",
    "inputs_train = prepare_data(proverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-23T20:51:29.946073Z",
     "iopub.status.busy": "2022-12-23T20:51:29.945672Z",
     "iopub.status.idle": "2022-12-23T20:56:16.671490Z",
     "shell.execute_reply": "2022-12-23T20:56:16.670366Z",
     "shell.execute_reply.started": "2022-12-23T20:51:29.946035Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3108\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1950\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1950' max='1950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1950/1950 04:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to out/checkpoint-500\n",
      "Configuration saved in out/checkpoint-500/config.json\n",
      "Model weights saved in out/checkpoint-500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "Saving model checkpoint to out/checkpoint-1000\n",
      "Configuration saved in out/checkpoint-1000/config.json\n",
      "Model weights saved in out/checkpoint-1000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "Saving model checkpoint to out/checkpoint-1500\n",
      "Configuration saved in out/checkpoint-1500/config.json\n",
      "Model weights saved in out/checkpoint-1500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1950, training_loss=0.06975801174457257, metrics={'train_runtime': 286.6873, 'train_samples_per_second': 108.411, 'train_steps_per_second': 6.802, 'total_flos': 511725031211520.0, 'train_loss': 0.06975801174457257, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LoaderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    #Entrainement du modèle avec notre jeu de données.\n",
    "datasets = LoaderDataset(inputs_train)\n",
    "args = TrainingArguments(\n",
    "    report_to='none',\n",
    "    output_dir='out',\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    prediction_loss_only=False,\n",
    "    do_train=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=datasets,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-23T20:56:16.676563Z",
     "iopub.status.busy": "2022-12-23T20:56:16.674756Z",
     "iopub.status.idle": "2022-12-23T20:56:19.556727Z",
     "shell.execute_reply": "2022-12-23T20:56:19.555650Z",
     "shell.execute_reply.started": "2022-12-23T20:56:16.676525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision après fine tuning est : 53.57142857142857\n"
     ]
    }
   ],
   "source": [
    "tests_proverbs = load_tests(\"data/test_proverbes.json\")\n",
    "word_predictions = get_predictions(tests_proverbs, model, tokenizer)\n",
    "right_words = get_solutions(tests_proverbs)\n",
    "print(f\"La précision après fine tuning est : {precision_metric(right_words, word_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Discussions**\n",
    "Avant le fine-tuning, le modèle a une précision de 35%. Cependant, après le fine-tuning, la précision est à 55%. Cela peut s'expliquer par le fait que sans le fine-tuning, le modèle peut rencontrer des tokens du jeu de test dont il n'avait pas vu lors de l'entrainement. De ce fait, lors de la prédiction d'un mot, le modèle lui attribuera un label pouvant être erroné. Pour éviter ce problème, il n'est nécessaire de faire du fine-tuning. Ce dernier permet d'entrainer le modèle avec notre corpus ce qui permettrait au modèle de faire le moins d'erreur lors du test.\n",
    "En conclusion, nous pouvons dire que le fine-tuning permet un réajustement du modèle pour un meilleur apprentissage du corpus et par conséquent une meilleure prédiction.\n",
    "En perspective, il serait intéressant de faire un fine-tuning sur les différentes composantes (les couches) du modèle pour explorer leurs influences sur la performance de BERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
